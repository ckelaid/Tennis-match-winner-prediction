---
title: "data_preprocess"
author: "Carlos Kelaidis"
date: "7/21/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
dat <- read.csv("~/Documents/My_working_directory/Personal_projects/Maine_tennis/tennis_match_data_combined.csv")

#View(dat)
```


```{r}
summary(dat)
```

# Normalize data

Only need to normalize count data, as percentage data is already in a [0,1] range:
```{r}
# Define Min Max function
Min_Max <- function(a){
  # a is an array
  for(i in 1:length(a)) {
    a[i] <- round((a[i] - min(a)) / (max(a) - min(a)), 2)
  }
  
  return(a)
}
```

```{r}
# Normalize data
dat$ACES <- Min_Max(dat$ACES)
dat$DOUBLE.FAULTS <- Min_Max(dat$DOUBLE.FAULTS)
dat$WINNERS <- Min_Max(dat$WINNERS)
dat$UNFORCED.ERRORS <- Min_Max(dat$UNFORCED.ERRORS)
```

# Variable Selection

- BSS
- Forward/Backward SS
- Lasso

1st we remove TOTAL.POINTS.WON.. from our data (dat_101 is without that variable), as this variable gives out too much info, and with the data we have, we could technically just use that to predict the winner (we do not have any data yet were the winner won less points than the loser ==> need to gather more data for that)
```{r}
dat_101 <- dat[,-12]
```

### BSS
```{r}
library(leaps)
library(glmnet)
bss.fit<-regsubsets(as.factor(WON.LOST)~., data=dat_101, nvmax=14)
sum.bss.fit<-summary(bss.fit)
sum.bss.fit

```

Let's observe our quality measures:
```{r}
par(mfrow=c(1,3))
# adrj2
plot(sum.bss.fit$adjr2, xlab="Number of Predictors",ylab="Adjusted R^2", type="l")+
  points(which.max(sum.bss.fit$adjr2), sum.bss.fit$adjr2[which.max(sum.bss.fit$adjr2)], 
         col=2, cex=2, pch=20)
  abline(h=max(sum.bss.fit$adjr2)+.2*sd(sum.bss.fit$adjr2), col=2, lty=2)
  abline(h=max(sum.bss.fit$adjr2)-.2*sd(sum.bss.fit$adjr2), col=2, lty=2)
# Cp
plot(sum.bss.fit$cp, xlab="Number of Predictors",ylab="Cp", type="l")+
  points(which.min(sum.bss.fit$cp), sum.bss.fit$cp[which.min(sum.bss.fit$cp)], col=2, cex=2, pch=20)
  abline(h=min(sum.bss.fit$cp)+.2*sd(sum.bss.fit$cp), col=2, lty=2)
  abline(h=min(sum.bss.fit$cp)-.2*sd(sum.bss.fit$cp), col=2, lty=2)
# BIC
plot(sum.bss.fit$bic, xlab="Number of Predictors",ylab="BIC", type="l")+
  points(which.min(sum.bss.fit$bic), sum.bss.fit$bic[which.min(sum.bss.fit$bic)], col=2, cex=2, pch=20)
  abline(h=min(sum.bss.fit$bic)+.2*sd(sum.bss.fit$bic), col=2, lty=2)
  abline(h=min(sum.bss.fit$bic)-.2*sd(sum.bss.fit$bic), col=2, lty=2)
```

Although adjr2, Cp and BIC propose varying numbers of variables, we do not pick 8, as it is outside the bounds (0.2 sd from optimum) for BIC. Using a model with 2 predictors might be too simple, so we opt for a 4 variable model, maybe a 6 or 5 variable model, this is still to be decided.

Hence, using BSS we chose a 4, 5 or 6 variable model, we print them below:
```{r}
coef(bss.fit, 4)
```

```{r}
coef(bss.fit, 5)
```

```{r}
coef(bss.fit, 6)
```


Ok so they all agree on the following variables:

- DOUBLE.FAULTS, X1st.SERVE.., SERVICE.POINTS.WON.., RETURN.POINTS.WON..

- The 5 var model introduces ACES

- The 6 var model adds X2nd.SERVE.PTS.WON..

### Lasso

Use CV to pick the lambda value:
```{r}
library(glmnet)
#Create dat.mat 
dat_mat_101<-model.matrix(as.factor(WON.LOST)~., data=dat_101)

#Create a grid for all possible lambda values
grid<-10^seq(10,-2, length=100)
set.seed(1)
lasso.cv.out<-cv.glmnet(dat_mat_101, dat_101$WON.LOST, alpha=1)
plot(lasso.cv.out)
```


```{r}
lasso.fit<-glmnet(dat_mat_101, dat_101$WON.LOST, alpha=1, lambda=grid, thresh=1e-12)
predict(lasso.fit, type="coefficients", s=lasso.cv.out$lambda.min)#[1:13,]
```

We see Lasso chooses a 7 variable model with the following variables:
- ACES, DOUBLE.FAULTS, X1st.SERVE.., X2nd.SERVE.PTS.WON.., UNFORCED.ERRORS, SERVICE.POINTS.WON.., RETURN.POINTS.WON..

Let's see what the BSS 7 variable model looks like:
```{r}
coef(bss.fit, 7)
```

Same model as Lasso

Ok so maybe our best pick is a 7 variable model. What we really ought to do is evaluate the 4, 5, 6 and 7 variable model and see which one performs the best.


### Forward SS


